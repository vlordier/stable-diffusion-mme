{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84cc22c",
   "metadata": {},
   "source": [
    "# Introduction to the SageMaker Large Model Inference (LMI) Container \n",
    "### Deploy a Stable Diffusion model on a SageMaker multi-model endpoint with LMI\n",
    "\n",
    "In this notebook, we explore how to host multiple Stable Diffusion models behind a Multi-Model Endpoint on SageMaker using the [Large Model Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) container. This container is optimized for hosting large models using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "This notebook was tested on a `ml.g5.xlarge` SageMaker Notebook instance using the `conda_pytorch_p39` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f899bc2",
   "metadata": {},
   "source": [
    "## Save a pre-trained Stable Diffusion model to deploy\n",
    "As a first step, we'll import the relevant libraries, configure several global variables such as the hosting image that will be used and the S3 location of our model artifacts. We also download the model weights from the [Diffusers](https://huggingface.co/docs/diffusers/index) library and save them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274590-2828-4592-8b01-219797b226a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 diffusers --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9515a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jinja2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffef362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"large-model-djl-sd/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"hf-large-model-djl-sd/model\"  # folder where model checkpoint will go\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51cb8d-3cd3-4eb8-b142-6e63b0ef7bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "We get the container image for the Large Model Inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070f3ba-12e4-41d8-9cd6-9a415b00b8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.21.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4b0c4-6a81-447f-8038-ef01bf5da3f9",
   "metadata": {},
   "source": [
    "The latest version of the LMI container does not include PyTorch 2.0, which has a bunch of new, useful optimization features; let's extend the public container and push that to ECR so that we are able to accelerate the Diffusion model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76393b1d-5b50-466e-8fe8-fa2e47dc2c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_image_name = 'extended-djl-deepspeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04041dd7-6495-49d1-b1da-12ae65a4f4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Often there are missing ECR permissions errors thrown when pulling public DLC's,\n",
    "so I first login to the public image's ECR repo\n",
    "\"\"\"\n",
    "public_acct_number = inference_image_uri.split('.')[0]\n",
    "!aws ecr get-login-password --region \"$region\" | docker login --username AWS --password-stdin \"$public_acct_number\".dkr.ecr.\"$region\".amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fb85e-2557-4d2a-bde7-cf0c2d79232f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "!cd docker && bash build_and_push.sh \"$new_image_name\" 0.21.0 \"$inference_image_uri\" \"$region\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84b4a3-6070-4ff7-a11e-a0e653e84261",
   "metadata": {},
   "source": [
    "The next cell checks if the container build process went OK, and retrieves the URI for our new container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a4449d2-be75-43e9-aefa-e07c49130bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'Error response from daemon' in str(build_output):\n",
    "    print(build_output)\n",
    "    raise SystemExit('\\n\\n!!There was an error with the container build!!')\n",
    "else:\n",
    "    extended_djl_image_uri = str(build_output).strip().split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9b6f3-ba1c-40cb-adeb-064e7dbd520e",
   "metadata": {},
   "source": [
    "We define a base URL location for our pretrained model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038346a8-d75b-410a-8d5d-fbb748e874d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_location = f's3://{bucket}/{s3_model_prefix}/sd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddfb5c-2073-4ac3-af8b-a6aa7b094cd2",
   "metadata": {},
   "source": [
    "Grab a pre-trained Stable Diffusion v1.4 model in fp16 precision (faster inference than fp32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454177f-3d75-48b1-892d-ca4d7e063ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch \n",
    "\n",
    "pipeline = diffusers.DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",\n",
    "                                                             cache_dir='hf_cache',\n",
    "                                                             torch_dtype=torch.float16,\n",
    "                                                             revision=\"fp16\",\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f52b8d-0f9d-46c4-a874-9331808f0ea4",
   "metadata": {},
   "source": [
    "Save the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4b59a-f014-49f4-b7a2-e02e2a21a2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.save_pretrained('stable_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc354b0-f789-4502-96a9-2831d6c25746",
   "metadata": {},
   "source": [
    "Now, we simulate having several different finetuned stable diffusion models to serve on S3, by copying the same model artifacts to several different S3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99396ec1-5464-43dc-8ab8-dd9be6df66d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_models = 4\n",
    "\n",
    "for i in range(n_models):\n",
    "    str_idx = str(i)\n",
    "    !aws s3 cp --recursive stable_diff/ \"$pretrained_model_location\"/sd-\"$str_idx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276822a-c1ee-4441-aeb6-bf2f642fcced",
   "metadata": {},
   "source": [
    "Check out all your models on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60caa83f-ce5f-47a0-94b2-0ef5ae3a8dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls \"$pretrained_model_location\"/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af980378",
   "metadata": {},
   "source": [
    "## Deploying Stable diffusion Using HF Accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ad640-b2e2-4ea0-b6a3-e577a276c6fd",
   "metadata": {},
   "source": [
    "First, we create a `serving.properties` file for each model, which will specify which one of the LMI backends to use, specific backend configurations (see all [here](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)), the name of the inference handler to be used (if it is a custom script, it should be packaged along with the properties file, in the same directory) , and also the s3URL of the corresponding model artifacts.\n",
    "\n",
    "We then tar each code directory (properties file + inference script) and upload it to S3. With other containers, Sagemaker downloads the .tar.gz file containing your model artifact and your inference code. With the LMI container, MME works a bit differently: the artifact that is automatically downloaded by SageMaker is actually the code artifact .tar.gz; within that tar, the `serving.properties` file holds the S3url information of the uncompressed model artifacts, which it is then the container's (in reality, the serving software inside it) responsibility to download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f41d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_models):\n",
    "    template = jinja_env.from_string(Path(\"accelerate_src/serving.template\").open().read())\n",
    "    Path(\"accelerate_src/serving.properties\").open(\"w\").write(\n",
    "        template.render(s3url=f'{pretrained_model_location}/sd-{i}/')\n",
    "    )\n",
    "    !pygmentize accelerate_src/serving.properties | cat -n\n",
    "\n",
    "    !tar czvf acc_model\"$i\".tar.gz accelerate_src/\n",
    "\n",
    "    ds_s3_code_artifact = sess.upload_data(f'acc_model{i}.tar.gz', bucket, s3_code_prefix)\n",
    "    print(f\"S3 Code or Model tar ball uploaded to --- > {ds_s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f167f-d610-4cc1-8dc8-45f2a5843f4a",
   "metadata": {},
   "source": [
    "Notice that for each configuration file, we don't specify an [option.entryPoint](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html#realtime-endpoints-large-model-configuration-general); by default, the server will look for a `model.py` file located in the same directory as the properties file (the naming convention is necessary).\n",
    "\n",
    "The inference code will be the same for all of the deployed models. You can understand the required structure for the inference handler [here](https://docs.djl.ai/docs/serving/serving/docs/modes.html#python-mode), and see other example inference handlers [here](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b8446-63c4-4fad-801e-4a8a4f6d766f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize accelerate_src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b83fd-e747-441d-82be-9dd16fbc001e",
   "metadata": {},
   "source": [
    "## SageMaker multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89452701-7b6f-462e-bbd5-f32ae4ca5340",
   "metadata": {},
   "source": [
    "Having uploaded all our artifacts to S3, we will now deploy a SageMaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be679d58-d8f4-422b-b7da-b03c23239517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf7a17-13f8-4256-9b1c-5197012c450c",
   "metadata": {},
   "source": [
    "Create a MultiDataModel, deploy it, and instantiate a Predictor to run predictions against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1dd167b-df8b-4f9b-9461-831cc94b8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "ds_endpoint_name = sagemaker.utils.name_from_base(\"lmi-mme-test\")\n",
    "\n",
    "model = MultiDataModel(ds_endpoint_name,\n",
    "                       # This is where all the code tar.gz files are located with LMI, not the model artifacts\n",
    "                       f's3://{bucket}/{s3_code_prefix}/',\n",
    "                       image_uri=inference_image_uri,\n",
    "                       role=role)\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=\"ml.g5.xlarge\",\n",
    "             endpoint_name=ds_endpoint_name)\n",
    "\n",
    "# our requests will be in json format, and responses as PNG Bytes, so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=ds_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.BytesDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00679f6-1619-4310-a7a9-259b8a64a335",
   "metadata": {
    "tags": []
   },
   "source": [
    "Invoke each model one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d41958-7703-4d59-8e35-406d4becd995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(n_models):\n",
    "    start = time.time()\n",
    "    response = predictor.predict(\n",
    "        {\"prompt\": \"astronaut spending money at a luxury store\",},target_model=f'acc_model{i}.tar.gz'\n",
    "    )\n",
    "\n",
    "    print(f'Took {time.time()-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb869c6-7799-4ae1-a0dc-1bd3bbc0b600",
   "metadata": {},
   "source": [
    "Decode the last response and show the image corresponding to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5cd56-b1f1-4996-b72c-5fed911d2818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "stream = BytesIO(a)\n",
    "image = Image.open(stream).convert(\"RGBA\")\n",
    "stream.close()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2303a-392c-4805-9328-48834bae8cca",
   "metadata": {},
   "source": [
    "Shut down the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223c87e-7668-4a94-b1ca-4435f7ccc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
